# BEVFusion in CARLA

![demo](assets/carla-demo.gif)

<p align="center"> <b>Left</b>: BEVFusion predictions <b>Right</b>: ground truth </p>

## About

This is a fork of [BEVFusion](https://github.com/mit-han-lab/bevfusion) that uses data from the [CARLA Simulator](https://github.com/carla-simulator/carla) generated by [SimBEV](https://github.com/GoodarzMehr/SimBEV) to train the model.

## Installation on Ubuntu

### Without Using Docker

The code is built with following libraries:

- Python >= 3.8, \<3.9
- OpenMPI = 4.0.3 and mpi4py = 3.0.3 (Needed for torchpack)
- Pillow = 8.4.0 (see [here](https://github.com/mit-han-lab/bevfusion/issues/63))
- [PyTorch](https://github.com/pytorch/pytorch) >= 1.9, \<= 1.10.2
- [tqdm](https://github.com/tqdm/tqdm)
- [torchpack](https://github.com/mit-han-lab/torchpack)
- [mmcv](https://github.com/open-mmlab/mmcv) = 1.4.0
- [mmdetection](http://github.com/open-mmlab/mmdetection) = 2.20.0
- [nuscenes-dev-kit](https://github.com/nutonomy/nuscenes-devkit)

After installing these dependencies, please run this command to install the codebase:
```bash
python setup.py develop
```

### Using Docker

1. Install [Docker](https://docs.docker.com/engine/install/) on your system.
2. Install the [Nvidia Container Toolkit](https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/install-guide.html#installation-guide). It exposes your Nvidia graphics card to Docker containers.
3. In the `docker` directory, run
```bash
docker build --no-cache --rm -t bevfusion:develop .
```
You may need to replace `libnvidia-gl-470` and `libnvidia-common-470` packages with ones that are compatible with your Nvidia driver version.

The following build arguments are optional:
* `USER`: username inside each container, set to `bf` by default.

## Usage

If you are using Docker, launch a container by running
```bash
docker run --privileged --gpus all --network=host -e DISPLAY=$DISPLAY
-v [path/to/BEVFusion]/bevfusion:/home/bevfusion
-v [path/to/dataset]/data:/dataset
--shm-size 32g -it bevfusion:develop /bin/bash
```
Then, in the `bevfusion` directory, run the following command to install the codebase
```bash
python setup.py develop
```

To train the model, run
```bash
torchpack dist-run -np 8 python tools/train.py configs/carla/seg/fusion-bev256d2-lss.yaml --model.encoders.camera.backbone.init_cfg.checkpoint pretrained/swint-nuimages-pretrained.pth
```
and replace 8 with the number of your GPUs. You can change `samples_per_gpu` and `workers_per_gpu` values in `configs/carla/default.yaml` based on your GPU memory and CPU cores.

For evaluation and visualization, change `data.test.ann_file` in `configs/carla/default.yaml` to `${dataset_root + "infos/simbev_infos_test.json"}`.

For evaluation, run
```bash
torchpack dist-run -np 8 python tools/test.py configs/carla/seg/fusion-bev256d2-lss.yaml pretrained/carla-bevfusion-seg.pth --eval map
```

For visualization, run
```bash
torchpack dist-run -np 8 python tools/visualize.py configs/carla/seg/fusion-bev256d2-lss.yaml --mode carla --checkpoint pretrained/carla-bevfusion-seg.pth --split test
```

The commands above train, test, and visualize the lidar-camera model. For camera-only or lidar-only models, use the appropriate configuration file.

## Results

The following table shows the IoU values of the lidar-camera model for different classes at three different thresholds.

| Threshold | Road | Car | Truck | Pedestrian |
| :-------: | :--: | :-: | :---: | :--------: |
| 0.3 | 92.6 | 74.7 | 79.1 | 28.1 |
| 0.5 | 92.7 | 76.2 | 83.3 | 27.4 |
| 0.7 | 92.4 | 75.5 | 82.9 | 11.8 |
